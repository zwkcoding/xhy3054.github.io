---
layout: post
title: ResNet论文笔记
date: 2018-06-25 10:07:24.000000000 +09:00
img:   pier.jpg # Add image post (optional)
tags: [机器学习]
---
## 退化问题
2013年以来cnn飞速发展，从比较简单的LeNet、AlexNet,到结构对称美观的VGGNet（经典结构之一，SegNet、SSD等都使用它作为基础网络），再到可以大量减少参数的GoogleNet（Inception），一直到本文主角ResNet的出现，它们都有一个共同的特点：网络层数在不断加深。

按理说，深度cnn网络集成了特征提取器与特征分类器，并且特征水平可以通过层数的堆砌来提升。层数越深，网络的学习能力应该越强，网络的准确度也应该越高，那只要不断加深cnn网络的层数不就好了？确实是这样的，但是，总归没有那么简单。
![issue]({{site.baseurl}}/assets/img/resnet/issue.png)

从上图我们可以看到，56层网络的训练错误率与测试错误率反而比20层的（在CIFAR-10上）要高。没错，**传统cnn网络随心所欲的增加层数是不行的,当更深的网络开始收敛时，会出现退化问题。（层数增多，准确率反而下降，因为深层网络会出现梯度消失）**

## ResNet的做法
ResNet为了解决cnn网络层数增加，学习能力反而下降的问题，提出了一种 residual 结构。
![residual]({{site.baseurl}}/assets/img/resnet/residual.png)

如上图，增加了一个恒等映射（identity mapping），将原始需要学习的函数`H(X)`转换成`F(x)+x`。作者认为对残差`F(x)`做优化会比直接对`H(x)`做优化要简单很多。这个公式的实现是通过上图的`shortcut connections`，这并不会增加额外参数和计算复杂性。

其中，`H(x) = y = F(x)+x`,`F(x)`为要学习的残差映射。但是，我们都知道输入与输出的维度很有可能会发生变化，因此`shortcut`的方式，需要认真考虑。针对深度增加的情况，论文中提出了两种解决方法：
1. zero_padding:依然是恒等映射，对于增加的层填充0,这样不会有额外的参数;
2. projection:采用`1×1`卷积核来增加维度。

## 实际效果
如下图，为普通 VGG-19 网络与34层普通cnn网络还有34层的残差网络结构图：
![net]({{site.baseurl}}/assets/img/resnet/net.png)

结果对比

![result]({{site.baseurl}}/assets/img/resnet/result.png)

## 改进
我们可以发现， ResNet 确实有效的解决了深度网络的退化问题。

但是虽然 ResNet 在深度网络的退化问题上取得了一定的效果，但是参数数量是不变的。这使得 ResNet 工作在更高的层数时依然吃力。为了减少参数数量，提高效率，作者又提出了以下结构：

![newres]({{site.baseurl}}/assets/img/resnet/newres.png)

使用`1×1`卷积核来分别进行降维与升维，使得`3×3`卷积核工作在较低维度，大大减少参数数量，作者基于此又提出了 ResNet 的50、101、152层网络。

## ResNet原理探讨
纵观全文，作者在引入新的结构时几乎没有进行原理上的分析，用到最多的词反而是：实验结果表明。ResNet是不可解释的吗？其实不然，下面就将尝试进行解释。

深层网络会出现梯度消失的问题，进而导致退化现象的产生。梯度消失的出现主要是由神经网络的求解方式---梯度下降法导致的，在进行梯度链式求导时，由于每一层梯度求导的值都会很小，当层数很深依次链式求导时，就会出现梯度消失。

而残差网络中，会有一条identity分支。`H(x) = F(x) + x`在反向传播求导(此处是对x求导)时，会得到`(1 + F'(x)) d H(x)`，此式可以保证与`d H(x)`保持在一个数量级，从而有效的减小了梯度消失。

另外，有一篇论文认为残差网络就是大量的不同长度的神经网络组成的组合函数，从而可以有效缓解梯度消失。
