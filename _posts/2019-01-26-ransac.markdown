---
layout: post
title: RANSAC算法的理解与使用
date: 2019-01-26 10:07:24.000000000 +09:00
img:  game4.jpg # Add image post (optional)
tag: [计算机视觉, 机器学习]
---
**随机抽样一致算法(random sample consensus, RANSAC)**，其实就是采用迭代的方法从一组包含离群的被观测数据中估算出数学模型的参数。(比如通过一群点拟合一条直线等)

## 基本假设
- 模型假设：事先知道真实数据满足的数学模型，不知道的只是模型的具体参数；

- 输入假设：输入数据中包含正确数据（即inliers，可以被假设模型描述的数据）；同时也包含异常数据（即outliers，偏离正常范围很远、不能用假设模型描述的数据）；即输入数据集中含有噪声；

- 有解假设：给定一组(通常很小，比如直线模型两个点就够了)内群数据，存在一种解法，可以通过这组内群数据求解出最适用于这一数据模型的一组参数；

> 一般当有效数据占绝大多数，无效数据只是少量时，我们可以通过**最小二乘法**或类似的方法来确定模型的参数与误差；如果无效数据很多，最小二乘法可能就不适用了

> 最小二乘法通过**最小化误差的平方和**来寻找数据的最佳函数匹配。

## 算法步骤概述
1. 在数据中随机选择几个点设定为内群；
2. 计算适合此内群的一组模型参数；
3. 把刚才没有选到的点带入刚才建立的模型中，计算误差的平方和（或者统计输入内群点的数量）；
4. 重复以上1、2、3步骤多次；
5. 选取其中使误差平方和最小的（或者内群点数量最多的）的那组模型参数作为我们要求的解；

## 参数选择
上述算法步骤中最重要的参数就是迭代的次数k了，这个参数的确定可以通过如下推导理解一下：

- 假设任取一个点是内群点的概率为w，则有`w = 数据中内群点的数量/数据中点的总数`；

- 则任取n个点都是内群点的概率为$w^n$(此处假设了数据中点的总数远大于n)；

- 所以我们所选择的n个点至少有一个不是内群点的概率为$1 - w^n$；

- 所以我们连续重复k次都不能有一次全是内群点的概率$p_e$为$(1 - w^n)^k$;

> 由上，我们发现当w保持不变时，我们要想让$p_e$尽量小，则n越大，k就需要越大。

## 在关键点匹配对筛选中的应用
- 模型假设：模型是单应矩阵表示的对极几何模型；
- 输入假设：输入的数据中包含正确的匹配(inliers)与误匹配(outliers);
- 有解假设：可以通过四对匹配特征点求解处自由度为8的单应矩阵；

> 具体代码效果可以见[我的github]()，不过我直接调用了opencv自带的ransac的实现，并没有自己手写。

# 参考资料
- [1] https://blog.csdn.net/laobai1015/article/details/51682596
- [2] https://zh.wikipedia.org/wiki/%E6%9C%80%E5%B0%8F%E4%BA%8C%E4%B9%98%E6%B3%95
- [3] https://docs.opencv.org/3.4.1/d9/d0c/group__calib3d.html#gae420abc34eaa03d0c6a67359609d8429




