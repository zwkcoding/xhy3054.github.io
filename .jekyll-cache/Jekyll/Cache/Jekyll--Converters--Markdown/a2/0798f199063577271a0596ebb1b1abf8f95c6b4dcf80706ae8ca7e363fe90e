I"	8<p>之前有仔细介绍过非线性优化，其中二阶梯度的方法有高斯牛顿法、LM算法、dogleg算法等。前段时间仔细研究了dogleg算法的实现步骤，今天抽出时间总结一下。</p>

<blockquote>
  <p>dogleg属于二阶梯度法，为了解决高斯牛顿法在使用一阶导数平方近似计算hessian矩阵时可能不准确的问题，它结合了高斯牛顿法与最速下降法，通过定义近似半径来限制近似的可执行范围。本质上与LM算法类似，不过实现细节大有不同。</p>
</blockquote>

<h2 id="算法">算法</h2>
<p>dogleg的思想很简单，对于一个非线性优化问题，它的每次的迭代步骤可以归纳如下：</p>

<ol>
  <li>
    <p>首先计算当前其高斯牛顿方法的迭代步长<script type="math/tex">H_{gn}</script>，然后计算最速下降法的迭代步长<script type="math/tex">H_{sd}</script>。</p>
  </li>
  <li>
    <p>根据第一步中确定的<script type="math/tex">H_{gn}</script>与<script type="math/tex">H_{sd}</script>相对与当前信赖域半径<script type="math/tex">r</script>的大小确定本次dogleg更新的步长<script type="math/tex">\delta</script>；</p>
  </li>
  <li>
    <p>使用第二步中确定的更新步长进行更新，然后计算$\rho$（与LM中相同），根据其大小判断是否执行本次迭代与如何更新信赖域半径<script type="math/tex">r</script>。</p>
  </li>
</ol>

<h3 id="第一步">第一步</h3>
<p>第一步是计算高斯牛顿与最速下降两种方法的更新步长。</p>

<ul>
  <li>高斯牛顿法步长<script type="math/tex">H_{gn}</script>可以通过求解如下方程组得到（推导可见<a href="https://xhy3054.github.io/nonlinear-optimization/">非线性优化</a>）</li>
</ul>

<script type="math/tex; mode=display">\left(\mathbf{J}(\mathbf{x})^{\top} \mathbf{J}(\mathbf{x})\right) \mathbf{h}_{\mathrm{gn}}=-\mathbf{J}(\mathbf{x})^{\top} \mathbf{f}(\mathbf{x})</script>

<ul>
  <li>最速下降法步长<script type="math/tex">H_{sd}</script>，获得可以分为两步来获得
    <ul>
      <li>第一步是先获得最速下降的方向</li>
      <li>第二步是确定最速下降的步长</li>
      <li>最后综合方向与步长即可得到更新的向量</li>
    </ul>
  </li>
</ul>

<p>首先，最速下降法的方向即是梯度反方向，先不考虑模长，我们可以使用下面公式表示(其中ｇ表示梯度方向)</p>

<script type="math/tex; mode=display">\mathbf{h}_{\mathrm{sd}} = -\mathbf{g} = -\mathbf{J}(\mathbf{x})^{\top} \mathbf{f}(\mathbf{x})</script>

<p>然后我们知道利用雅克比矩阵可以得到下面模型：</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{aligned} \mathbf{f}\left(\mathbf{x}+\alpha \mathbf{h}_{\mathrm{sd}}\right) & \simeq \mathbf{f}(\mathbf{x})+\alpha \mathbf{J}(\mathbf{x}) \mathbf{h}_{\mathrm{sd}} \\ F\left(\mathbf{x}+\alpha \mathbf{h}_{\mathrm{sd}}\right) & \simeq \frac{1}{2}\left\|\mathbf{f}(\mathbf{x})+\alpha \mathbf{J}(\mathbf{x}) \mathbf{h}_{\mathrm{sd}}\right\|^{2} \\ &=F(\mathbf{x})+\alpha \mathbf{h}_{\mathrm{sd}}^{\top} \mathbf{J}(\mathbf{x})^{\top} \mathbf{f}(\mathbf{x})+\frac{1}{2} \alpha^{2}\left\|\mathbf{J}(\mathbf{x}) \mathbf{h}_{\mathrm{sd}}\right\|^{2} \end{aligned} %]]></script>

<p>我们对上述模型求最小值，此时<script type="math/tex">\alpha</script>应取值为：</p>

<script type="math/tex; mode=display">\alpha=-\frac{\mathbf{h}_{\mathrm{sd}}^{\top} \mathbf{J}(\mathbf{x})^{\top} \mathbf{f}(\mathbf{x})}{\left\|\mathbf{J}(\mathbf{x}) \mathbf{h}_{\mathrm{sd}}\right\|^{2}}=\frac{\|\mathbf{g}\|^{2}}{\|\mathbf{J}(\mathbf{x}) \mathbf{g}\|^{2}}</script>

<p>所以，最后最速下降法的更新向量为</p>

<p><script type="math/tex">\mathbf{h}_{\mathrm{sd}} = \alpha \mathbf{h}_{\mathrm{sd}}</script>　</p>

<h3 id="第二步">第二步</h3>
<p>通过当前的信赖域半径与上一步计算的两种步长结合计算处dogleg算法的更新步长更新规则如下：</p>
<div style="text-align: center">
<img src="/assets/img/math/dogleg.PNG" />
</div>
<p>上图中的更新规则描述的较为清楚，但是代码实现尚有难度，主要是<script type="math/tex">\beta</script>的值较难确定。我在网上看到了一个更加明确的规则，将上述更新规则总结为一个统一的表达式。</p>

<script type="math/tex; mode=display">% <![CDATA[
h_{dl}=\left\{\begin{array}{ll}{\tau h_{sd},} & {0 \leq \tau \leq 1} \\ {h_{sd}+(\tau-1)\left(h_{gn}-h_{sd}\right),} & {1 \leq \tau \leq 2}\end{array}\right. %]]></script>

<p>其中比较关键的是变量<script type="math/tex">\tau</script>的确定，我直接贴出代码(其中<code class="highlighter-rouge">delta_</code>是信赖区域的半径大小)</p>

<div class="language-cpp highlighter-rouge"><div class="highlight"><pre class="highlight"><code>            <span class="kt">double</span> <span class="n">tau</span><span class="p">;</span>
            <span class="k">if</span><span class="p">(</span><span class="n">H_gn</span><span class="p">.</span><span class="n">norm</span><span class="p">()</span> <span class="o">&lt;=</span> <span class="n">delta_</span><span class="p">)</span>
                <span class="n">tau</span> <span class="o">=</span> <span class="mi">2</span><span class="p">;</span>
            <span class="k">else</span> <span class="nf">if</span><span class="p">(</span><span class="n">H_sd</span><span class="p">.</span><span class="n">norm</span><span class="p">()</span> <span class="o">&gt;=</span> <span class="n">delta_</span><span class="p">)</span>
                <span class="n">tau</span> <span class="o">=</span> <span class="n">delta_</span> <span class="o">/</span> <span class="n">H_sd</span><span class="p">.</span><span class="n">norm</span><span class="p">()</span> <span class="p">;</span>
            <span class="k">else</span><span class="p">{</span>
                <span class="n">VecX</span> <span class="n">tmp</span> <span class="o">=</span> <span class="n">H_gn</span> <span class="o">-</span> <span class="n">H_sd</span> <span class="p">;</span>
                <span class="n">tau</span> <span class="o">=</span> <span class="p">(</span><span class="o">-</span> <span class="n">H_sd</span><span class="p">.</span><span class="n">transpose</span><span class="p">()</span> <span class="o">*</span> <span class="n">tmp</span> <span class="o">+</span> <span class="n">sqrt</span><span class="p">(</span><span class="n">pow</span><span class="p">((</span><span class="n">H_sd</span><span class="p">.</span><span class="n">transpose</span><span class="p">()</span> <span class="o">*</span> <span class="n">tmp</span><span class="p">),</span><span class="mi">2</span><span class="p">)</span> <span class="o">-</span> <span class="n">tmp</span><span class="p">.</span><span class="n">squaredNorm</span><span class="p">()</span> <span class="o">*</span> <span class="p">(</span><span class="n">H_sd</span><span class="p">.</span><span class="n">squaredNorm</span><span class="p">()</span> <span class="o">-</span> <span class="n">pow</span><span class="p">(</span><span class="n">delta_</span><span class="p">,</span><span class="mi">2</span><span class="p">))))</span> <span class="o">/</span> <span class="n">tmp</span><span class="p">.</span><span class="n">squaredNorm</span><span class="p">()</span>       <span class="p">;</span>
                <span class="n">tau</span> <span class="o">+=</span> <span class="mi">1</span><span class="p">;</span>     
            <span class="p">}</span>
</code></pre></div></div>
<hr />

<h3 id="第三步">第三步</h3>
<p>判断改更新是否执行，并更新信赖域半径，此处的迭代策略比较简单，我是看的参考文献第一篇提供的策略，直接贴代码了。</p>

<p>注意此处有个<script type="math/tex">\rho</script>的计算，其值为</p>

<script type="math/tex; mode=display">\rho=\frac{F(\mathbf{x})-F\left(\mathbf{x}+\Delta \mathbf{x}_{\operatorname{dl}}\right)}{L(\mathbf{0})-L\left(\Delta \mathbf{x}_{\mathrm{dl}}\right)}</script>

<p>其中分母是此次更新后真实目标函数损失减少的量，分子是使用近似模型此次更新后目标函数损失应该减少的量。其中<script type="math/tex">L(h)</script>函数是如下线性函数模型</p>

<script type="math/tex; mode=display">L(\mathbf{h}) = \frac{1}{2}\|\mathbf{f}(\mathbf{x})+\mathbf{J}(\mathbf{x}) \mathbf{h}\|^{2}</script>

<div class="language-cpp highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1">// 狗腿</span>
<span class="kt">bool</span> <span class="n">Problem</span><span class="o">::</span><span class="n">IsGoodStep</span><span class="p">()</span> <span class="p">{</span>
    <span class="c1">//此处是计算当前目标函数的损失</span>
    <span class="kt">double</span> <span class="n">tempChi</span> <span class="o">=</span> <span class="mf">0.0</span><span class="p">;</span>
    <span class="k">for</span><span class="p">(</span><span class="k">auto</span> <span class="n">edge</span><span class="o">:</span><span class="n">edges_</span><span class="p">){</span>
        <span class="n">edge</span><span class="p">.</span><span class="n">second</span><span class="o">-&gt;</span><span class="n">ComputeResidual</span><span class="p">();</span>
        <span class="n">tempChi</span> <span class="o">+=</span> <span class="n">edge</span><span class="p">.</span><span class="n">second</span><span class="o">-&gt;</span><span class="n">Chi2</span><span class="p">();</span>    
    <span class="p">}</span>
 
    <span class="c1">//此处是计算近似使用dogleg步长更新理论上应该减小的损失</span>
    <span class="kt">double</span> <span class="n">scale</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span>
    <span class="n">scale</span> <span class="o">=</span> <span class="n">delta_x_</span><span class="p">.</span><span class="n">transpose</span><span class="p">()</span> <span class="o">*</span> <span class="n">b_</span> <span class="p">;</span>
    <span class="n">std</span><span class="o">::</span><span class="n">cout</span><span class="o">&lt;&lt;</span><span class="s">"第一部分scale为 "</span><span class="o">&lt;&lt;</span><span class="n">scale</span><span class="o">&lt;&lt;</span><span class="n">endl</span><span class="p">;</span>
    <span class="n">scale</span> <span class="o">-=</span> <span class="mf">0.5</span> <span class="o">*</span> <span class="n">delta_x_</span><span class="p">.</span><span class="n">transpose</span><span class="p">()</span> <span class="o">*</span> <span class="n">Hessian_</span> <span class="o">*</span> <span class="n">delta_x_</span><span class="p">;</span> 
    <span class="n">scale</span> <span class="o">+=</span> <span class="mf">1e-3</span><span class="p">;</span>
    <span class="n">cout</span><span class="o">&lt;&lt;</span><span class="s">"scale = "</span><span class="o">&lt;&lt;</span><span class="n">scale</span><span class="o">&lt;&lt;</span><span class="n">endl</span><span class="p">;</span>

    <span class="kt">double</span> <span class="n">rho</span> <span class="o">=</span> <span class="p">(</span><span class="n">currentChi_</span> <span class="o">-</span> <span class="n">tempChi</span><span class="p">)</span><span class="o">/</span><span class="n">scale</span><span class="p">;</span>
    <span class="n">std</span><span class="o">::</span><span class="n">cout</span> <span class="o">&lt;&lt;</span> <span class="s">"rho: "</span> <span class="o">&lt;&lt;</span> <span class="n">rho</span> <span class="o">&lt;&lt;</span> <span class="s">" , chi= "</span> <span class="o">&lt;&lt;</span> <span class="n">tempChi</span> <span class="o">&lt;&lt;</span> <span class="s">" , delta= "</span> <span class="o">&lt;&lt;</span> <span class="n">delta_</span>
                  <span class="o">&lt;&lt;</span> <span class="n">std</span><span class="o">::</span><span class="n">endl</span><span class="p">;</span>
    <span class="c1">//迭代策略</span>
    <span class="k">if</span><span class="p">(</span><span class="n">rho</span> <span class="o">&gt;</span> <span class="mi">0</span> <span class="o">&amp;&amp;</span> <span class="n">isfinite</span><span class="p">(</span><span class="n">tempChi</span><span class="p">)){</span>
        <span class="k">if</span><span class="p">(</span><span class="n">rho</span><span class="o">&gt;</span><span class="mf">0.75</span><span class="p">)</span> <span class="n">delta_</span> <span class="o">=</span> <span class="n">std</span><span class="o">::</span><span class="n">max</span><span class="p">(</span><span class="n">delta_</span><span class="p">,</span> <span class="mf">3.</span><span class="o">*</span><span class="n">delta_x_</span><span class="p">.</span><span class="n">norm</span><span class="p">());</span>
        <span class="k">else</span> <span class="k">if</span><span class="p">(</span><span class="n">rho</span><span class="o">&lt;</span><span class="mf">0.25</span><span class="p">)</span>   <span class="n">delta_</span> <span class="o">=</span> <span class="n">delta_</span><span class="o">/</span><span class="mf">2.</span><span class="p">;</span>
        <span class="n">currentChi_</span> <span class="o">=</span> <span class="n">tempChi</span><span class="p">;</span>
        <span class="k">return</span> <span class="nb">true</span><span class="p">;</span>    
    <span class="p">}</span>
    <span class="k">else</span><span class="p">{</span>
        <span class="n">delta_</span> <span class="o">=</span> <span class="n">delta_</span><span class="o">/</span><span class="mf">2.</span><span class="p">;</span>
        <span class="k">return</span> <span class="nb">false</span><span class="p">;</span>    
    <span class="p">}</span>

<span class="p">}</span>

</code></pre></div></div>

<h2 id="实现">实现</h2>
<p>对于dogleg算法基于eigen实现了一般版本，下面是其应用于曲线拟合的实例。</p>

<h1 id="参考文献">参考文献</h1>
<ul>
  <li>
    <p>[1] METHODS FOR NON-LINEAR LEAST SOUARES PROBLEMS. K.Madsen, H.B.Nielsen, O.Tingleff</p>
  </li>
  <li>
    <p>[2] Is Levenberg-Marquardt the Most Efficient Optimization Algorithm for Implementing Bundle Adjustment. Manolis I.A.</p>
  </li>
</ul>

:ET