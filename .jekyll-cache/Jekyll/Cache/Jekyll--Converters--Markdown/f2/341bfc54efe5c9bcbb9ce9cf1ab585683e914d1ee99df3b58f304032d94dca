I"<p>对pytorch中组成网络的各种结构进行介绍记录，防止忘掉。</p>

<h2 id="2维卷积层torchnnconv2d">2维卷积层<code class="highlighter-rouge">torch.nn.Conv2d</code></h2>

<blockquote>
  <p>卷积操作主要用来提取图像特征，卷积过程可以参考我之前的一篇<a href="https://xhy3054.github.io/cnn-base/">文章</a>中的一个动图。</p>
</blockquote>

<div class="language-py highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">CLASS</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span>	<span class="n">in_channels</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> 
				<span class="n">out_channels</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> 
				<span class="n">kernel_size</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> 
				<span class="n">Tuple</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="nb">int</span><span class="p">]],</span> 
				<span class="n">stride</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="n">Tuple</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="nb">int</span><span class="p">]]</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span> 
				<span class="n">padding</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="n">Tuple</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="nb">int</span><span class="p">]]</span> <span class="o">=</span> <span class="mi">0</span><span class="p">,</span> 
				<span class="n">dilation</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="n">Tuple</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="nb">int</span><span class="p">]]</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span> 
				<span class="n">groups</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span> 
				<span class="n">bias</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="bp">True</span><span class="p">,</span> 
				<span class="n">padding_mode</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s">'zeros'</span><span class="p">)</span>
</code></pre></div></div>

<ul>
  <li><code class="highlighter-rouge">in_channels</code>：一个整型，表示输入通道数；</li>
  <li><code class="highlighter-rouge">out_channels</code>：一个整型，表示输出通道数；</li>
  <li><code class="highlighter-rouge">kernel_size</code>：卷积核尺寸，可以是一个整型数，也可以是一个包含两个整型的tuple，一个整数时默认长宽相同；</li>
  <li><code class="highlighter-rouge">stride</code>： 输入可以是一个整型号，也可以是一个包含两个整型的tuple；表示步长；一般步长也可以看作提取到的特征相对于原图尺寸缩小的倍数；</li>
  <li><code class="highlighter-rouge">padding</code>：输入可以是一个整型号，也可以是一个包含两个整型的tuple；表示默认在图像边缘补0的尺寸，输入一个tuple的话，第一个数表示高度上，第二个表示宽度上填充的尺寸；比如当为1时，表示原图基础上，上下左右各补了一行；</li>
  <li><code class="highlighter-rouge">dilation</code>：输入可以是一个整型号，也可以是一个包含两个整型的tuple；表示卷积核的扩张尺寸，也是核点之间的间距；</li>
  <li><code class="highlighter-rouge">groups</code>: 范围从1到<code class="highlighter-rouge">in_channels</code>，默认是1，表示所有每个输出通道都包含了所有输入通道的信息；如果为2，则一半的输出通道是由一半的输入通道产生的，另一半的输出通道是由剩下一半的输入通道产生；如果为<code class="highlighter-rouge">in_channels</code>，表示每个输入通道都有自己单独的卷积核，卷积核输出维度为<code class="highlighter-rouge">out_channels/in_channels</code>；</li>
</ul>

<h2 id="批处理归一化层torchnnbatchnorm2d">批处理归一化层<code class="highlighter-rouge">torch.nn.BatchNorm2d</code></h2>

<blockquote>
  <p>批处理归一化层主要用来进行数据归一化，使得在relu之前不会因数据过大而导致网络性能不稳定。对于一个（1,3，H,W）的输入，主要的处理如下：</p>
</blockquote>

<script type="math/tex; mode=display">y=\frac{x-\mathrm{E}[x]}{\sqrt{\operatorname{Var}[x]+\epsilon}} * \gamma+\beta</script>

<p>上面计算是对3个通道分别进行的，其中x为一个一个像素位置多个通道值组成的向量，E为每个通道的平均数组成的向量，var为每个通道的标准差组成的向量，$\epsilon$是为了数值稳定性在分母上增加的值，$\gamma$（默认1）和$beta$（默认0）为可学习的参数向量（维度为通道数）。</p>

<div class="language-py highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">CLASS</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">BatchNorm2d</span><span class="p">(</span><span class="n">num_features</span><span class="p">,</span> 
	<span class="n">eps</span><span class="o">=</span><span class="mf">1e-05</span><span class="p">,</span> 
	<span class="n">momentum</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> 
	<span class="n">affine</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> 
	<span class="n">track_running_stats</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
</code></pre></div></div>

<ul>
  <li><code class="highlighter-rouge">num_features</code>：当前要处理的特征层的通道数（一开始rgb图像的话是3）；</li>
  <li><code class="highlighter-rouge">eps</code>：也就是上面公式中的$\epsilon$，为数值稳定性在分母上增加的值。默认为：1e-5；</li>
  <li><code class="highlighter-rouge">momentum</code>：一个用于运行过程中均值和方差的一个估计参数（我的理解是一个稳定系数，类似于SGD中的momentum的系数）；</li>
  <li><code class="highlighter-rouge">affine</code>：当设为true时，该模块拥有可以学习的系数$\gamma$和$beta$；</li>
  <li><code class="highlighter-rouge">track_running_stats</code>: 是否跟踪均值和标准差的计算；</li>
</ul>
:ET