I"j><p>对pytorch中组成网络的各种结构进行介绍记录，防止忘掉。</p>

<h1 id="提取并调整特征维度">提取并调整特征维度</h1>

<h2 id="2维卷积层torchnnconv2d">2维卷积层<code class="highlighter-rouge">torch.nn.Conv2d</code></h2>

<blockquote>
  <p>卷积操作主要用来提取图像特征，卷积过程可以参考我之前的一篇<a href="https://xhy3054.github.io/cnn-base/">文章</a>中的一个动图。</p>
</blockquote>

<div class="language-py highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">CLASS</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span>	<span class="n">in_channels</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> 
				<span class="n">out_channels</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> 
				<span class="n">kernel_size</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> 
				<span class="n">Tuple</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="nb">int</span><span class="p">]],</span> 
				<span class="n">stride</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="n">Tuple</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="nb">int</span><span class="p">]]</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span> 
				<span class="n">padding</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="n">Tuple</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="nb">int</span><span class="p">]]</span> <span class="o">=</span> <span class="mi">0</span><span class="p">,</span> 
				<span class="n">dilation</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="n">Tuple</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="nb">int</span><span class="p">]]</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span> 
				<span class="n">groups</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span> 
				<span class="n">bias</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="bp">True</span><span class="p">,</span> 
				<span class="n">padding_mode</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s">'zeros'</span><span class="p">)</span>
</code></pre></div></div>

<ul>
  <li><code class="highlighter-rouge">in_channels</code>：一个整型，表示输入通道数；</li>
  <li><code class="highlighter-rouge">out_channels</code>：一个整型，表示输出通道数；</li>
  <li><code class="highlighter-rouge">kernel_size</code>：卷积核尺寸，可以是一个整型数，也可以是一个包含两个整型的tuple，一个整数时默认长宽相同；</li>
  <li><code class="highlighter-rouge">stride</code>： 输入可以是一个整型号，也可以是一个包含两个整型的tuple；表示步长；一般步长也可以看作提取到的特征相对于原图尺寸缩小的倍数；</li>
  <li><code class="highlighter-rouge">padding</code>：输入可以是一个整型号，也可以是一个包含两个整型的tuple；表示默认在图像边缘补0的尺寸，输入一个tuple的话，第一个数表示高度上，第二个表示宽度上填充的尺寸；比如当为1时，表示原图基础上，上下左右各补了一行；</li>
  <li><code class="highlighter-rouge">dilation</code>：输入可以是一个整型号，也可以是一个包含两个整型的tuple；表示卷积核的扩张尺寸，也是核点之间的间距；</li>
  <li><code class="highlighter-rouge">groups</code>: 范围从1到<code class="highlighter-rouge">in_channels</code>，默认是1，表示所有每个输出通道都包含了所有输入通道的信息；如果为2，则一半的输出通道是由一半的输入通道产生的，另一半的输出通道是由剩下一半的输入通道产生；如果为<code class="highlighter-rouge">in_channels</code>，表示每个输入通道都有自己单独的卷积核，卷积核输出维度为<code class="highlighter-rouge">out_channels/in_channels</code>；</li>
</ul>

<h2 id="上采样层torchnnpixelshuffle">上采样层<code class="highlighter-rouge">torch.nn.PixelShuffle</code></h2>

<blockquote>
  <p>主要用来重新排列一个维度(x, C<em>r^2, H, W)的tensor为一个维度(x, C, H</em>r, W<em>r)的tensor。这种做法是16年一篇论文提出的超分辨率方法，主要思想是对于（1, H, W）维度的图像，为了将其放大r倍，首先使用卷积网络提取得到(r^2, H, W)维度的深层特征；然后使用PixelShuffle对深层特征重新排列，得到一个(1, H</em>r, W*r)的高分辨的图像。</p>
</blockquote>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">CLASS</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">PixelShuffle</span><span class="p">(</span><span class="n">upscale_factor</span><span class="p">:</span> <span class="nb">int</span><span class="p">)</span>
</code></pre></div></div>

<ul>
  <li><code class="highlighter-rouge">upscale_factor</code>：int类型，上采样的倍数，也是上面的r；</li>
</ul>

<h1 id="归一化调整数值范围层">归一化调整数值范围层</h1>

<h2 id="批处理归一化层torchnnbatchnorm2d">批处理归一化层<code class="highlighter-rouge">torch.nn.BatchNorm2d</code></h2>

<blockquote>
  <p>批处理归一化层主要用来进行数据归一化，使得在relu之前不会因数据过大而导致网络性能不稳定。对于一个（1,3，H,W）的输入，主要的处理如下：</p>
</blockquote>

<script type="math/tex; mode=display">y=\frac{x-\mathrm{E}[x]}{\sqrt{\operatorname{Var}[x]+\epsilon}} * \gamma+\beta</script>

<p>上面计算是对3个通道分别进行的，其中x为一个一个像素位置多个通道值组成的向量，E为每个通道的平均数组成的向量，var为每个通道的标准差组成的向量，$\epsilon$是为了数值稳定性在分母上增加的值，$\gamma$（默认1）和$beta$（默认0）为可学习的参数向量（维度为通道数）。上面公式写成向量形式，但是实际理解可以是对特征的每个通道分别执行上式。</p>

<div class="language-py highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">CLASS</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">BatchNorm2d</span><span class="p">(</span><span class="n">num_features</span><span class="p">,</span> 
	<span class="n">eps</span><span class="o">=</span><span class="mf">1e-05</span><span class="p">,</span> 
	<span class="n">momentum</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> 
	<span class="n">affine</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> 
	<span class="n">track_running_stats</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
</code></pre></div></div>

<ul>
  <li><code class="highlighter-rouge">num_features</code>：当前要处理的特征层的通道数（一开始rgb图像的话是3）；</li>
  <li><code class="highlighter-rouge">eps</code>：也就是上面公式中的$\epsilon$，为数值稳定性在分母上增加的值。默认为：1e-5；</li>
  <li><code class="highlighter-rouge">momentum</code>：一个用于运行过程中均值和方差的一个估计参数（我的理解是一个稳定系数，类似于SGD中的momentum的系数）；</li>
  <li><code class="highlighter-rouge">affine</code>：当设为true时，该模块拥有可以学习的系数$\gamma$和$beta$；</li>
  <li><code class="highlighter-rouge">track_running_stats</code>: 是否跟踪均值和标准差的计算；</li>
</ul>

<h2 id="归一化层torchnnfunctionalnormalize">归一化层<code class="highlighter-rouge">torch.nn.functional.normalize</code></h2>

<blockquote>
  <p>在指定的维度上执行$L_p$归一化操作，其实就是在某个维度上对每个元素做$L_p$操作并做sum，然后当前元素值为原值除以sum，公式为：</p>
</blockquote>

<script type="math/tex; mode=display">v=\frac{v}{\max \left(\|v\|_{p}, \epsilon\right)}</script>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">functional</span><span class="o">.</span><span class="n">normalize</span><span class="p">(</span><span class="nb">input</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> 
	<span class="n">p</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mi">2</span><span class="p">,</span> 
	<span class="n">dim</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span> 
	<span class="n">eps</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">1e-12</span><span class="p">,</span> 
	<span class="n">out</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="bp">None</span><span class="p">)</span> <span class="err">→</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span>
</code></pre></div></div>

<ul>
  <li><code class="highlighter-rouge">input</code>：任意shape的tensor输入；</li>
  <li><code class="highlighter-rouge">p</code>：范数的指数，默认为2，即$L_2$范数；</li>
  <li><code class="highlighter-rouge">dim</code>：计算范数的维度，默认是1；</li>
  <li><code class="highlighter-rouge">out</code>：the output tensor. If out is used, this operation won’t be differentiable.</li>
</ul>

<h2 id="层torchnnfunctionalsoftplus">层<code class="highlighter-rouge">torch.nn.functional.softplus</code></h2>

<blockquote>
  <p>逐元素执行如下操作，note:为了数值稳定，当$\beta x &gt; threshold$时实现变为线性函数。</p>
</blockquote>

<script type="math/tex; mode=display">\text { Softplus }(x)=\frac{1}{\beta} * \log (1+\exp (\beta * x))</script>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">functional</span><span class="o">.</span><span class="n">softplus</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">beta</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">threshold</span><span class="o">=</span><span class="mi">20</span><span class="p">)</span> <span class="err">→</span> <span class="n">Tensor</span>
</code></pre></div></div>

<h2 id="层torchnnfunctionalsoftmax">层<code class="highlighter-rouge">torch.nn.functional.softmax</code></h2>

<blockquote>
  <p>也是一种归一化操作，沿着某个维度执行如下操作，使得该维度向量每个元素值在0到1之间，并且和为1。</p>
</blockquote>

<script type="math/tex; mode=display">\operatorname{Softmax}\left(x_{i}\right)=\frac{\exp \left(x_{i}\right)}{\sum_{j} \exp \left(x_{j}\right)}</script>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">functional</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="nb">input</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> 
	<span class="n">dim</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="bp">None</span><span class="p">,</span> 
	<span class="n">_stacklevel</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">3</span><span class="p">,</span> 
	<span class="n">dtype</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="bp">None</span><span class="p">)</span> <span class="err">→</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span>
</code></pre></div></div>

<ul>
  <li><code class="highlighter-rouge">input</code>：输入tensor；</li>
  <li><code class="highlighter-rouge">dim</code>：沿着这个维度执行softmax</li>
  <li><code class="highlighter-rouge">dtype</code>：如果指定类型，在执行操作前需要将输入被转换为该类型；</li>
</ul>

<h1 id="非线性激活函数层">非线性激活函数层</h1>

<h2 id="修正线性单元层torchnnrelu">修正线性单元层<code class="highlighter-rouge">torch.nn.ReLU</code></h2>

<blockquote>
  <p>主要是执行非线性变换。Relu的非线性变换是大于0时不变，小于0时置为0，特点是瘦脸快，求梯度简单。</p>
</blockquote>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">CLASStorch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">(</span><span class="n">inplace</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="bp">False</span><span class="p">)</span>
</code></pre></div></div>

<ul>
  <li><code class="highlighter-rouge">inplace</code>：是否在原数据上操作；</li>
</ul>

<h2 id="第二种激活函数torchnnelu">第二种激活函数<code class="highlighter-rouge">torch.nn.ELU</code></h2>

<blockquote>
  <p>对每个元素执行如下操作：</p>
</blockquote>

<script type="math/tex; mode=display">ELU(x)=max(0,x)+min(0,α∗(exp(x)−1))</script>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">CLASStorch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">ELU</span><span class="p">(</span><span class="n">alpha</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">1.0</span><span class="p">,</span> <span class="n">inplace</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="bp">False</span><span class="p">)</span>
</code></pre></div></div>

<ul>
  <li><code class="highlighter-rouge">alpha</code>：上面公式中的$\alpha$值，默认为1；</li>
  <li><code class="highlighter-rouge">inplace</code>：是否在原数据位置进行操作；</li>
</ul>

<h1 id="参考文献">参考文献</h1>

<ul>
  <li>[1] pytorch官方文档</li>
</ul>

:ET